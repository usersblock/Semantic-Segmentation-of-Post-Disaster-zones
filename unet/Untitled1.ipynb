{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce635d8c-3127-4a01-85c6-bd710d1bff97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:67: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:67: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "C:\\Users\\thanh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'segmentation_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Activation\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msegmentation_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m freeze_model\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msegmentation_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m legacy_support\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msegmentation_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbones\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_backbone, get_feature_layers\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'segmentation_models'"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D\n",
    "from keras.layers import Activation\n",
    "from keras.models import Model\n",
    "\n",
    "from segmentation_models.utils import freeze_model\n",
    "from segmentation_models.utils import legacy_support\n",
    "from segmentation_models.backbones import get_backbone, get_feature_layers\n",
    "\n",
    "from segmentation_models.unet.blocks import Transpose2D_block\n",
    "from segmentation_models.utils import get_layer_number, to_tuple\n",
    "\n",
    "from keras.layers import Concatenate\n",
    "from segmentation_models.unet.blocks import UpSampling2D, handle_block_names, ConvRelu\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input\n",
    "from keras.models import load_model\n",
    "\n",
    "old_args_map = {\n",
    "    'freeze_encoder': 'encoder_freeze',\n",
    "    'skip_connections': 'encoder_features',\n",
    "    'upsample_rates': None,  # removed\n",
    "    'input_tensor': None,  # removed\n",
    "}\n",
    "\n",
    "\n",
    "@legacy_support(old_args_map)\n",
    "def SiameseUnet(backbone_name='vgg16',\n",
    "         input_shape=(None, None, 3),\n",
    "         classes=1,\n",
    "         activation='sigmoid',\n",
    "         encoder_weights='imagenet',\n",
    "         encoder_freeze=False,\n",
    "         encoder_features='default',\n",
    "         decoder_block_type='upsampling',\n",
    "         decoder_filters=(256, 128, 64, 32, 16),\n",
    "         decoder_use_batchnorm=True,\n",
    "         **kwargs):\n",
    "    \"\"\" Unet_ is a fully convolution neural network for image semantic segmentation\n",
    "        Args:\n",
    "            backbone_name: name of classification model (without last dense layers) used as feature\n",
    "                extractor to build segmentation model.\n",
    "            input_shape: shape of input data/image ``(H, W, C)``, in general\n",
    "                case you do not need to set ``H`` and ``W`` shapes, just pass ``(None, None, C)`` to make your model be\n",
    "                able to process images af any size, but ``H`` and ``W`` of input images should be divisible by factor ``32``.\n",
    "            classes: a number of classes for output (output shape - ``(h, w, classes)``).\n",
    "            activation: name of one of ``keras.activations`` for last model layer\n",
    "                (e.g. ``sigmoid``, ``softmax``, ``linear``).\n",
    "            encoder_weights: one of ``None`` (random initialization), ``imagenet`` (pre-training on ImageNet).\n",
    "            encoder_freeze: if ``True`` set all layers of encoder (backbone model) as non-trainable.\n",
    "            encoder_features: a list of layer numbers or names starting from top of the model.\n",
    "                Each of these layers will be concatenated with corresponding decoder block. If ``default`` is used\n",
    "                layer names are taken from ``DEFAULT_SKIP_CONNECTIONS``.\n",
    "            decoder_block_type: one of blocks with following layers structure:\n",
    "                - `upsampling`:  ``Upsampling2D`` -> ``Conv2D`` -> ``Conv2D``\n",
    "                - `transpose`:   ``Transpose2D`` -> ``Conv2D``\n",
    "            decoder_filters: list of numbers of ``Conv2D`` layer filters in decoder blocks\n",
    "            decoder_use_batchnorm: if ``True``, ``BatchNormalisation`` layer between ``Conv2D`` and ``Activation`` layers\n",
    "                is used.\n",
    "        Returns:\n",
    "            ``keras.models.Model``: **Unet**\n",
    "        .. _Unet:\n",
    "            https://arxiv.org/pdf/1505.04597\n",
    "    \"\"\"\n",
    "\n",
    "    load_weights_from = None\n",
    "    if encoder_weights is not \"imagenet\" and encoder_weights is not None:\n",
    "        load_weights_from = encoder_weights\n",
    "        encoder_weights = None\n",
    "\n",
    "\n",
    "    backbone = get_backbone(backbone_name,\n",
    "                            input_shape=input_shape,\n",
    "                            input_tensor=None,\n",
    "                            weights=encoder_weights,\n",
    "                            include_top=False)\n",
    "\n",
    "    if load_weights_from is not None:\n",
    "        model_to_load_weights_from = load_model(load_weights_from)\n",
    "\n",
    "        # now let's assume that this loaded model had its own \"top\" upsampling section trained on another task\n",
    "        # let's transplant what we can, that is the backbone encoder\n",
    "\n",
    "        output = model_to_load_weights_from.layers[len(backbone.layers)-1].output  # remove activation and last conv layer\n",
    "        transplant = keras.models.Model(model_to_load_weights_from.input, output)\n",
    "        #transplant.summary()\n",
    "\n",
    "        transplant.save(\"transplant.h5\") # hacky way\n",
    "        backbone.load_weights(\"transplant.h5\")\n",
    "\n",
    "        # Check if the weights have been loaded\n",
    "        \"\"\"\n",
    "        inspect_i = 0\n",
    "        import numpy as np\n",
    "        w1 = np.asarray(transplant.get_weights()[inspect_i])\n",
    "        print(w1)\n",
    "        w2 = np.asarray(backbone.get_weights()[inspect_i])\n",
    "        print(w2)\n",
    "        \"\"\"\n",
    "        print(\"Loaded weights into \",backbone_name,\"from\",load_weights_from)\n",
    "\n",
    "    if encoder_features == 'default':\n",
    "        encoder_features = get_feature_layers(backbone_name, n=4)\n",
    "\n",
    "    model = build_siamese_unet(backbone,\n",
    "                       classes,\n",
    "                       encoder_features,\n",
    "                       decoder_filters=decoder_filters,\n",
    "                       block_type=decoder_block_type,\n",
    "                       activation=activation,\n",
    "                       n_upsample_blocks=len(decoder_filters),\n",
    "                       upsample_rates=(2, 2, 2, 2, 2),\n",
    "                       use_batchnorm=decoder_use_batchnorm,\n",
    "                       input_shape=input_shape)\n",
    "\n",
    "    # lock encoder weights for fine-tuning\n",
    "    if encoder_freeze:\n",
    "        freeze_model(backbone)\n",
    "\n",
    "    model.name = 'u-{}'.format(backbone_name)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def Siamese_Upsample2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n",
    "                     use_batchnorm=False, skip_a=None, skip_b=None):\n",
    "\n",
    "    def layer(input_tensor):\n",
    "\n",
    "        conv_name, bn_name, relu_name, up_name = handle_block_names(stage)\n",
    "\n",
    "        x = UpSampling2D(size=upsample_rate, name=up_name)(input_tensor)\n",
    "\n",
    "        if skip_a is not None and skip_b is not None:\n",
    "            x = Concatenate()([x, skip_a, skip_b]) # siamese concatenation\n",
    "\n",
    "        x = ConvRelu(filters, kernel_size, use_batchnorm=use_batchnorm,\n",
    "                     conv_name=conv_name + '1', bn_name=bn_name + '1', relu_name=relu_name + '1')(x)\n",
    "\n",
    "        x = ConvRelu(filters, kernel_size, use_batchnorm=use_batchnorm,\n",
    "                     conv_name=conv_name + '2', bn_name=bn_name + '2', relu_name=relu_name + '2')(x)\n",
    "\n",
    "        return x\n",
    "    return layer\n",
    "\n",
    "\n",
    "def build_siamese_unet(backbone, classes, skip_connection_layers,\n",
    "               decoder_filters=(256,128,64,32,16),\n",
    "               upsample_rates=(2,2,2,2,2),\n",
    "               n_upsample_blocks=5,\n",
    "               block_type='upsampling',\n",
    "               activation='sigmoid',\n",
    "               use_batchnorm=True,\n",
    "               input_shape=(None, None, 3)):\n",
    "\n",
    "    verbose = False\n",
    "    if verbose:\n",
    "        print(\"Entered build_unet with arguments:\")\n",
    "        print(\"backbone\",backbone)\n",
    "        #print(\"---\\n\")\n",
    "        #backbone.summary()\n",
    "        #print(\"---\\n\")\n",
    "\n",
    "\n",
    "        print(\"classes\",classes)\n",
    "        print(\"skip_connection_layers\",skip_connection_layers)\n",
    "        print(\"decoder_filters\",decoder_filters)\n",
    "        print(\"upsample_rates\",upsample_rates)\n",
    "        print(\"n_upsample_blocks\",n_upsample_blocks)\n",
    "        print(\"block_type\",block_type)\n",
    "        print(\"activation\",activation)\n",
    "        print(\"use_batchnorm\",use_batchnorm)\n",
    "\n",
    "    input = backbone.input\n",
    "    x = backbone.output\n",
    "\n",
    "    # Prepare for multiple heads in siamese nn:\n",
    "\n",
    "    skip_connection_idx = ([get_layer_number(backbone, l) if isinstance(l, str) else l\n",
    "                               for l in skip_connection_layers])\n",
    "    if verbose:\n",
    "        print(\"skip_connection_idx\", skip_connection_idx)\n",
    "\n",
    "    skip_connections = []\n",
    "    for idx in skip_connection_idx:\n",
    "        skip_connection = backbone.layers[idx].output\n",
    "        skip_connections.append(skip_connection)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"skip_connections layers\", len(skip_connections), skip_connections)\n",
    "    #4 layers\n",
    "    # 'stage4_unit1_relu1/Relu:0' shape=(?, 16, 16, 256)\n",
    "    # 'stage3_unit1_relu1/Relu:0' shape=(?, 32, 32, 128)\n",
    "    # 'stage2_unit1_relu1/Relu:0' shape=(?, 64, 64, 64)\n",
    "    # 'relu0/Relu:0'              shape=(?, 128, 128, 64)\n",
    "\n",
    "    siamese_backbone_model_encode = Model(inputs=[input], outputs=[x]+skip_connections)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"siamese_model_encode.input\", siamese_backbone_model_encode.input)\n",
    "        print(\"siamese_model_encode.output\", siamese_backbone_model_encode.output) # x and the (now 4) skip connections\n",
    "\n",
    "    # Then merging\n",
    "    input_a = Input(shape=(input_shape[0], input_shape[1], input_shape[2]))\n",
    "    input_b = Input(shape=(input_shape[0], input_shape[1], input_shape[2]))\n",
    "\n",
    "    branch_a_outputs = siamese_backbone_model_encode([input_a])\n",
    "    branch_b_outputs = siamese_backbone_model_encode([input_b])\n",
    "\n",
    "    branch_a = branch_a_outputs[0]\n",
    "    branch_b = branch_b_outputs[0]\n",
    "\n",
    "    x = Concatenate(name=\"concatHighLvlFeat\")([branch_a, branch_b]) # both inputs, in theory 8x8x512 + 8x8x512 -> 8x8x1024\n",
    "\n",
    "    skip_connection_outputs_a = branch_a_outputs[1:]\n",
    "    skip_connection_outputs_b = branch_b_outputs[1:]\n",
    "\n",
    "    if block_type == 'transpose':\n",
    "        up_block = Transpose2D_block\n",
    "        assert False # NOT IMPLEMENTED\n",
    "    else:\n",
    "        up_block = Siamese_Upsample2D_block\n",
    "\n",
    "    for i in range(n_upsample_blocks):\n",
    "        skip_connection_a = None\n",
    "        skip_connection_b = None\n",
    "        if i < len(skip_connection_idx): # also till len(skip_connection_outputs_a)\n",
    "            skip_connection_a = skip_connection_outputs_a[i]\n",
    "            skip_connection_b = skip_connection_outputs_b[i]\n",
    "\n",
    "        upsample_rate = to_tuple(upsample_rates[i])\n",
    "\n",
    "        x = up_block(decoder_filters[i], i, upsample_rate=upsample_rate,\n",
    "                     skip_a=skip_connection_a, skip_b=skip_connection_b, use_batchnorm=use_batchnorm)(x)\n",
    "\n",
    "    x = Conv2D(classes, (3,3), padding='same', name='final_conv')(x)\n",
    "    x = Activation(activation, name=activation)(x)\n",
    "\n",
    "    #model = Model(input, x)\n",
    "    full_model = Model(inputs=[input_a, input_b], outputs=x)\n",
    "\n",
    "    return full_model\n",
    "\n",
    "\n",
    "\n",
    "# There is support for all of these (with weights from ImageNet included) ... qubvel/segmentation_models is awesome!\n",
    "# VGG           'vgg16' 'vgg19'\n",
    "# ResNet\t    'resnet18' 'resnet34' 'resnet50' 'resnet101' 'resnet152'\n",
    "# SE-ResNet\t    'seresnet18' 'seresnet34' 'seresnet50' 'seresnet101' 'seresnet152'\n",
    "# ResNeXt\t    'resnext50' 'resnet101'\n",
    "# SE-ResNeXt\t'seresnext50' 'seresnet101'\n",
    "# SENet154\t    'senet154'\n",
    "# DenseNet\t    'densenet121' 'densenet169' 'densenet201'\n",
    "# Inception\t    'inceptionv3' 'inceptionresnetv2'\n",
    "# MobileNet\t    'mobilenet' 'mobilenetv2'\n",
    "# Performance comparison for classification: https://github.com/qubvel/classification_models\n",
    "\n",
    "\n",
    "BACKBONE = 'resnet34'\n",
    "custom_weights_file = \"model_UNet-Resnet34_DSM_in01_95percOfTrain_8batch_100ep_dsm01proper.h5\" # None\n",
    "custom_weights_file = \"imagenet\"\n",
    "model = SiameseUnet(BACKBONE, encoder_weights=custom_weights_file, classes=3, activation='softmax', input_shape=(256, 256, 3))\n",
    "print(\"Model loaded:\")\n",
    "print(\"model.input\", model.input)\n",
    "print(\"model.output\", model.output)\n",
    "\n",
    "#model.summary()\n",
    "\n",
    "# Ps: there is posibility to change the code of additional models in similar manner to get FPN, Linknet and PSPNet\n",
    "# Ps2: some of these Siamese NN models end up with large amount of parameters ...\n",
    "#      if we don't have much data, we should perhaps freeze some of the layers of the encoder... \"encoder_freeze=False\"\n",
    "\n",
    "# Ps3: keras saves models into $ cd /home/<username>/.keras/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a14c69-3b85-48dc-bec9-93a7e9bb8d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
